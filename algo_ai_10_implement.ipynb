{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7745dd-f32d-436d-989b-ba89b7a25824",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The 10 Most Popular AI Algorithms and How to Implement Them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03568eb4-ca26-4606-9fe3-0931d859370c",
   "metadata": {},
   "source": [
    "1. Linear Regression\n",
    "Overview\n",
    "Linear Regression is the simplest algorithm for predictive modeling. It estimates relationships between a dependent variable and one or more independent variables by fitting a straight line.\n",
    "\n",
    "Use Case\n",
    "Predicting housing prices based on square footage\n",
    "Sales forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5256ae7b-b34c-47fd-981c-c5dbae2fc60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 7.999999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3]])  # Independent variable\n",
    "y = np.array([2, 4, 6])  # Dependent variable\n",
    "# Model training\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "# Prediction\n",
    "prediction = model.predict([[4]])\n",
    "print(f\"Predicted value: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e917d-0c30-470c-bfc7-6c2bb4059d50",
   "metadata": {},
   "source": [
    "2. Logistic Regression\n",
    "Overview\n",
    "Despite its name, Logistic Regression is a classification algorithm. It’s best for binary classification problems.\n",
    "\n",
    "Use Case\n",
    "Spam email detection\n",
    "Predicting user churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb480a7-6c4a-44de-82ac-6d5a2d887595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "# Model training\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "# Prediction\n",
    "prediction = model.predict([[1.5]])\n",
    "print(f\"Predicted class: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6c83c-25a9-47dd-8f63-2cf758a6b792",
   "metadata": {},
   "source": [
    "3. Decision Trees\n",
    "Overview\n",
    "Decision Trees mimic human decision-making processes. They split data into subsets based on feature values.\n",
    "\n",
    "Use Case\n",
    "Credit risk assessment\n",
    "Customer segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7489553f-2e13-4c10-bf34-0972585a15f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "# Model training\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "# Prediction\n",
    "prediction = model.predict([[2, 2]])\n",
    "print(f\"Predicted class: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61d83d-11d9-43fe-8785-8d624757b6bd",
   "metadata": {},
   "source": [
    "4. Support Vector Machines (SVM)\n",
    "Overview\n",
    "SVMs classify data by finding the hyperplane that best separates classes.\n",
    "\n",
    "Use Case\n",
    "Image recognition\n",
    "Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe5d294-a9c2-445c-8718-6cb79ef1dcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "# Model training\n",
    "model = SVC()\n",
    "model.fit(X, y)\n",
    "# Prediction\n",
    "prediction = model.predict([[0.5, 0.5]])\n",
    "print(f\"Predicted class: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3821c56-ffa4-4105-9ae7-e4c66f9c4bec",
   "metadata": {},
   "source": [
    "5. K-Nearest Neighbors (KNN)\n",
    "Overview\n",
    "KNN is a simple, lazy algorithm that classifies data points based on the majority class of their nearest neighbors.\n",
    "\n",
    "Use Case\n",
    "Recommender systems\n",
    "Fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f7f9b4-3eb4-4786-86c4-992e397ed083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Sample data\n",
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "# Model training\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X, y)\n",
    "# Prediction\n",
    "prediction = model.predict([[1.5]])\n",
    "print(f\"Predicted class: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5bfefc-f652-49d7-81ca-436fa82eb264",
   "metadata": {},
   "source": [
    "6. Naive Bayes\n",
    "Overview\n",
    "Naive Bayes is a probabilistic algorithm based on Bayes’ Theorem. It’s especially effective for text classification.\n",
    "\n",
    "Use Case\n",
    "Sentiment analysis\n",
    "Spam filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66deb930-b222-400f-9bb1-9fc597c72b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [2, 3], [3, 4]]\n",
    "y = [0, 1, 0]\n",
    "# Model training\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "# Prediction\n",
    "prediction = model.predict([[2, 3]])\n",
    "print(f\"Predicted class: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e62190-a467-4e4f-85b7-6d368a7e8570",
   "metadata": {},
   "source": [
    "7. K-Means Clustering\n",
    "Overview\n",
    "K-Means is an unsupervised algorithm that partitions data into K clusters based on feature similarity.\n",
    "\n",
    "Use Case\n",
    "Market segmentation\n",
    "Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c05a28ec-d2a8-4c2f-b1eb-a7d9fa2f7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster assignments: [1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample data\n",
    "X = [[1], [2], [3], [10], [11], [12]]\n",
    "# Model training\n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(X)\n",
    "# Cluster assignments\n",
    "clusters = model.predict(X)\n",
    "print(f\"Cluster assignments: {clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908bf2b7-8b04-44df-a85c-88ec1ae07f93",
   "metadata": {},
   "source": [
    "8. Random Forest\n",
    "Overview\n",
    "Random Forest combines multiple decision trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "Use Case\n",
    "Fraud detection\n",
    "Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e71c04af-f987-4fda-8b79-c5297b28d72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "# Model training\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "# Prediction\n",
    "prediction = model.predict([[2, 2]])\n",
    "print(f\"Predicted class: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63b2c7-6329-4f74-b254-5f13840f7e67",
   "metadata": {},
   "source": [
    "9. Neural Networks\n",
    "Overview\n",
    "Neural Networks are the cornerstone of deep learning, designed to mimic the human brain’s neural connections.\n",
    "\n",
    "Use Case\n",
    "Image recognition\n",
    "Natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f190b262-0c58-47a5-82a7-10bddd86f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached protobuf-5.29.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached wrapt-1.17.0-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.68.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading numpy-2.0.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "   ---------------------------------------- 0.0/390.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 8.4/390.2 MB 43.2 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 18.6/390.2 MB 41.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 29.6/390.2 MB 45.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 41.7/390.2 MB 48.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 52.7/390.2 MB 49.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 65.3/390.2 MB 50.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 77.6/390.2 MB 51.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 87.6/390.2 MB 50.3 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 93.6/390.2 MB 47.8 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 98.0/390.2 MB 47.4 MB/s eta 0:00:07\n",
      "   ---------- ---------------------------- 103.8/390.2 MB 43.6 MB/s eta 0:00:07\n",
      "   ---------- ---------------------------- 108.3/390.2 MB 41.9 MB/s eta 0:00:07\n",
      "   ----------- --------------------------- 112.7/390.2 MB 40.2 MB/s eta 0:00:07\n",
      "   ----------- --------------------------- 115.1/390.2 MB 38.1 MB/s eta 0:00:08\n",
      "   ----------- --------------------------- 117.2/390.2 MB 36.3 MB/s eta 0:00:08\n",
      "   ------------ -------------------------- 122.4/390.2 MB 35.4 MB/s eta 0:00:08\n",
      "   ------------ -------------------------- 126.4/390.2 MB 34.5 MB/s eta 0:00:08\n",
      "   ------------ -------------------------- 130.0/390.2 MB 33.5 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 139.2/390.2 MB 33.8 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 149.2/390.2 MB 34.4 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 154.1/390.2 MB 33.8 MB/s eta 0:00:07\n",
      "   --------------- ----------------------- 156.5/390.2 MB 33.9 MB/s eta 0:00:07\n",
      "   --------------- ----------------------- 157.3/390.2 MB 31.5 MB/s eta 0:00:08\n",
      "   ---------------- ---------------------- 162.5/390.2 MB 31.2 MB/s eta 0:00:08\n",
      "   ---------------- ---------------------- 166.2/390.2 MB 30.7 MB/s eta 0:00:08\n",
      "   ----------------- --------------------- 179.3/390.2 MB 31.9 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 179.3/390.2 MB 31.9 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 183.5/390.2 MB 30.3 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 185.9/390.2 MB 30.3 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 189.8/390.2 MB 29.3 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 192.9/390.2 MB 28.8 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 195.6/390.2 MB 28.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 200.5/390.2 MB 28.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 202.6/390.2 MB 27.6 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 203.9/390.2 MB 26.9 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 206.0/390.2 MB 26.2 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 211.0/390.2 MB 26.1 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 227.0/390.2 MB 27.4 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 231.7/390.2 MB 27.2 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 247.7/390.2 MB 28.4 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 252.4/390.2 MB 28.2 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 255.9/390.2 MB 27.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 269.2/390.2 MB 28.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 277.1/390.2 MB 28.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 282.3/390.2 MB 28.1 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 300.4/390.2 MB 28.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 306.4/390.2 MB 27.7 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 315.1/390.2 MB 27.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 333.7/390.2 MB 27.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 340.3/390.2 MB 27.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 344.2/390.2 MB 27.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 346.0/390.2 MB 26.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 349.4/390.2 MB 26.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 353.6/390.2 MB 25.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 356.5/390.2 MB 25.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 357.6/390.2 MB 25.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 359.9/390.2 MB 24.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 369.1/390.2 MB 25.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.7/390.2 MB 26.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 27.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.2/390.2 MB 12.7 MB/s eta 0:00:00\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.1-cp311-cp311-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 4.2/4.4 MB 83.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 20.4 MB/s eta 0:00:00\n",
      "Downloading h5py-3.12.1-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 35.2 MB/s eta 0:00:00\n",
      "Using cached keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Downloading numpy-2.0.2-cp311-cp311-win_amd64.whl (15.9 MB)\n",
      "   ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 7.3/15.9 MB 34.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 36.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 36.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 36.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 36.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 36.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.9/15.9 MB 11.2 MB/s eta 0:00:00\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-5.29.2-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 19.7 MB/s eta 0:00:00\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wrapt-1.17.0-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-win_amd64.whl (292 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, h5py, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.12.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 h5py-3.12.1 keras-3.7.0 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.13.1 protobuf-5.29.2 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\sudwa\\anaconda3\\envs\\kedro\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\sudwa\\anaconda3\\envs\\kedro\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85fecb36-2659-4e04-88ef-1039acb2354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudwa\\anaconda3\\envs\\kedro\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=[[0], [1]] (of type <class 'list'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[0;32m     16\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([[\u001b[38;5;241m0.5\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kedro\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kedro\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:125\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[1;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized data type: x=[[0], [1]] (of type <class 'list'>)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Sample data\n",
    "X = [[0], [1]]\n",
    "y = [[0], [1]]\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Dense(units=1, input_dim=1, activation='sigmoid')\n",
    "])\n",
    "# Model compilation\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "# Model training\n",
    "model.fit(X, y, epochs=10)\n",
    "# Prediction\n",
    "prediction = model.predict([[0.5]])\n",
    "print(f\"Predicted value: {prediction[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20300683-10b9-47f5-a1a8-230ddfcbc9b5",
   "metadata": {},
   "source": [
    "10. Gradient Boosting (XGBoost)\n",
    "Overview\n",
    "Gradient Boosting builds trees sequentially, with each tree correcting errors from the previous one.\n",
    "\n",
    "Use Case\n",
    "Predictive analytics\n",
    "Recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc14cf22-a351-4895-8edd-5658f73b25ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sudwa\\anaconda3\\envs\\kedro\\lib\\site-packages (from xgboost) (1.14.1)\n",
      "Using cached xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e7865f-8e6e-4b3c-8978-74cbfabc37de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "y = [0, 1, 0]\n",
    "# Model training\n",
    "model = XGBClassifier()\n",
    "model.fit(X, y)\n",
    "# Prediction\n",
    "prediction = model.predict([[2, 3]])\n",
    "print(f\"Predicted class: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3725cc2-616c-4e06-ae5d-ca9703dfd5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
