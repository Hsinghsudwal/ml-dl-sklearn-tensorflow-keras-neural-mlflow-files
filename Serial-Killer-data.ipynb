{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d9b47e-6d4a-498a-bde6-88707d49d000",
   "metadata": {},
   "source": [
    "### How to Find a Serial Killer with Data. A Steps to Cleaning, Wrangling, and Feature Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500925d1-f344-4293-b3bd-cac4a3fff718",
   "metadata": {},
   "source": [
    "Use Case:\n",
    "\n",
    "Understanding crime patterns and trends is crucial for making data-driven decisions in law enforcement and policy-making. For this demonstration, we’ll leverage the Murder Accountability Project’s extensive dataset, which includes homicide information spanning decades in the United States. Our goal is to make this data clean, complete, and informative — suitable for predictive tasks like clearance rate prediction, offender profiling, and hotspot analysis. Each step we take in transforming this data will be designed to answer specific questions and create actionable insights for real-world applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e324aa-5aa8-4934-aa3f-8faeea505b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CNTYFIPS</th>\n",
       "      <th>Ori</th>\n",
       "      <th>State</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Agentype</th>\n",
       "      <th>Source</th>\n",
       "      <th>Solved</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>OffRace</th>\n",
       "      <th>OffEthnic</th>\n",
       "      <th>Weapon</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Circumstance</th>\n",
       "      <th>Subcircum</th>\n",
       "      <th>VicCount</th>\n",
       "      <th>OffCount</th>\n",
       "      <th>FileDate</th>\n",
       "      <th>MSA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197603001AK00101</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>AK00101</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>Municipal police</td>\n",
       "      <td>FBI</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1976</td>\n",
       "      <td>March</td>\n",
       "      <td>...</td>\n",
       "      <td>Black</td>\n",
       "      <td>Unknown or not reported</td>\n",
       "      <td>Handgun - pistol, revolver, etc</td>\n",
       "      <td>Relationship not determined</td>\n",
       "      <td>Other arguments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30180.0</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197604001AK00101</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>AK00101</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>Municipal police</td>\n",
       "      <td>FBI</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1976</td>\n",
       "      <td>April</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Unknown or not reported</td>\n",
       "      <td>Handgun - pistol, revolver, etc</td>\n",
       "      <td>Girlfriend</td>\n",
       "      <td>Other arguments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30180.0</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197606001AK00101</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>AK00101</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>Municipal police</td>\n",
       "      <td>FBI</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1976</td>\n",
       "      <td>June</td>\n",
       "      <td>...</td>\n",
       "      <td>Black</td>\n",
       "      <td>Unknown or not reported</td>\n",
       "      <td>Handgun - pistol, revolver, etc</td>\n",
       "      <td>Stranger</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30180.0</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197606002AK00101</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>AK00101</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>Municipal police</td>\n",
       "      <td>FBI</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1976</td>\n",
       "      <td>June</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Unknown or not reported</td>\n",
       "      <td>Handgun - pistol, revolver, etc</td>\n",
       "      <td>Other - known to victim</td>\n",
       "      <td>Other arguments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30180.0</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>197607001AK00101</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>AK00101</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>Municipal police</td>\n",
       "      <td>FBI</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1976</td>\n",
       "      <td>July</td>\n",
       "      <td>...</td>\n",
       "      <td>American Indian or Alaskan Native</td>\n",
       "      <td>Unknown or not reported</td>\n",
       "      <td>Knife or cutting instrument</td>\n",
       "      <td>Brother</td>\n",
       "      <td>Other arguments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30180.0</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID       CNTYFIPS      Ori   State     Agency  \\\n",
       "0  197603001AK00101  Anchorage, AK  AK00101  Alaska  Anchorage   \n",
       "1  197604001AK00101  Anchorage, AK  AK00101  Alaska  Anchorage   \n",
       "2  197606001AK00101  Anchorage, AK  AK00101  Alaska  Anchorage   \n",
       "3  197606002AK00101  Anchorage, AK  AK00101  Alaska  Anchorage   \n",
       "4  197607001AK00101  Anchorage, AK  AK00101  Alaska  Anchorage   \n",
       "\n",
       "           Agentype Source Solved  Year  Month  ...  \\\n",
       "0  Municipal police    FBI    Yes  1976  March  ...   \n",
       "1  Municipal police    FBI    Yes  1976  April  ...   \n",
       "2  Municipal police    FBI    Yes  1976   June  ...   \n",
       "3  Municipal police    FBI    Yes  1976   June  ...   \n",
       "4  Municipal police    FBI    Yes  1976   July  ...   \n",
       "\n",
       "                             OffRace                OffEthnic  \\\n",
       "0                              Black  Unknown or not reported   \n",
       "1                              White  Unknown or not reported   \n",
       "2                              Black  Unknown or not reported   \n",
       "3                              White  Unknown or not reported   \n",
       "4  American Indian or Alaskan Native  Unknown or not reported   \n",
       "\n",
       "                            Weapon                 Relationship  \\\n",
       "0  Handgun - pistol, revolver, etc  Relationship not determined   \n",
       "1  Handgun - pistol, revolver, etc                   Girlfriend   \n",
       "2  Handgun - pistol, revolver, etc                     Stranger   \n",
       "3  Handgun - pistol, revolver, etc      Other - known to victim   \n",
       "4      Knife or cutting instrument                      Brother   \n",
       "\n",
       "      Circumstance Subcircum VicCount OffCount  FileDate            MSA  \n",
       "0  Other arguments       NaN        0        0   30180.0  Anchorage, AK  \n",
       "1  Other arguments       NaN        0        0   30180.0  Anchorage, AK  \n",
       "2            Other       NaN        0        0   30180.0  Anchorage, AK  \n",
       "3  Other arguments       NaN        0        0   30180.0  Anchorage, AK  \n",
       "4  Other arguments       NaN        0        0   30180.0  Anchorage, AK  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('https://github.com/fenago/datasets/raw/refs/heads/main/SHR65_23.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d53f674-b61f-447e-84d1-a6d0cd31e658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing_Values</th>\n",
       "      <th>Unique_Values</th>\n",
       "      <th>Sample_Value_Counts</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>Highly_Correlated</th>\n",
       "      <th>High_Variance</th>\n",
       "      <th>Low_Variance</th>\n",
       "      <th>Potential_Index</th>\n",
       "      <th>Data_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>0</td>\n",
       "      <td>852394</td>\n",
       "      <td>[(197702001NY05904, 13), (199204001FL05003, 12...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNTYFIPS</th>\n",
       "      <td>0</td>\n",
       "      <td>3079</td>\n",
       "      <td>[(Los Angeles, CA, 55681), (New York, NY, 4780...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ori</th>\n",
       "      <td>0</td>\n",
       "      <td>13442</td>\n",
       "      <td>[(NY03030, 47790), (ILCPD00, 30509), (CA01942,...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>[(California, 129741), (Texas, 86288), (New Yo...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agency</th>\n",
       "      <td>0</td>\n",
       "      <td>10015</td>\n",
       "      <td>[(New York, 47790), (Chicago, 30509), (Los Ang...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agentype</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>[(Municipal police, 692613), (Sheriff, 150999)...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[(FBI, 855718), (MAP, 38918)]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Solved</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[(Yes, 632457), (No, 262179)]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>[(1993, 24337), (1992, 23793), (2021, 23699)]</td>\n",
       "      <td>1998.868617</td>\n",
       "      <td>14.134767</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>[(July, 82444), (August, 80644), (December, 76...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Incident</th>\n",
       "      <td>0</td>\n",
       "      <td>1805</td>\n",
       "      <td>[(1, 364086), (2, 96879), (3, 53334)]</td>\n",
       "      <td>53.117632</td>\n",
       "      <td>790.036562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27113.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ActionType</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[(Normal update, 743918), (Adjustment, 150718)]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Homicide</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[(Murder and non-negligent manslaughter, 87495...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Situation</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>[(Single victim/single offender, 489422), (Sin...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicAge</th>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>[(22, 31266), (25, 31172), (21, 30973)]</td>\n",
       "      <td>47.313424</td>\n",
       "      <td>117.434844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicSex</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(Male, 693403), (Female, 199567), (Unknown, 1...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicRace</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>[(White, 435824), (Black, 427294), (Asian, 127...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicEthnic</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(Unknown or not reported, 501898), (Not of Hi...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffAge</th>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>[(999, 296295), (20, 28727), (19, 27048)]</td>\n",
       "      <td>351.740167</td>\n",
       "      <td>455.602403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffSex</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(Male, 559134), (Unknown, 264353), (Female, 7...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffRace</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>[(Black, 304272), (White, 302973), (Unknown, 2...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffEthnic</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(Unknown or not reported, 628333), (Not of Hi...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weapon</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>[(Handgun - pistol, revolver, etc, 429715), (K...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relationship</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>[(Relationship not determined, 354036), (Acqua...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Circumstance</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>[(Circumstances undetermined, 249629), (Other ...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subcircum</th>\n",
       "      <td>859886</td>\n",
       "      <td>7</td>\n",
       "      <td>[(Felon killed in commission of a crime, 11949...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicCount</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>[(0, 816622), (1, 56960), (2, 12398)]</td>\n",
       "      <td>0.134561</td>\n",
       "      <td>0.605561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffCount</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>[(0, 780250), (1, 77211), (2, 22934)]</td>\n",
       "      <td>0.194908</td>\n",
       "      <td>0.617827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FileDate</th>\n",
       "      <td>17075</td>\n",
       "      <td>6244</td>\n",
       "      <td>[(30180.0, 77684), (81723.0, 28487), (92324.0,...</td>\n",
       "      <td>54137.984079</td>\n",
       "      <td>32779.720549</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>30180.0</td>\n",
       "      <td>41317.0</td>\n",
       "      <td>82207.0</td>\n",
       "      <td>123197.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSA</th>\n",
       "      <td>0</td>\n",
       "      <td>409</td>\n",
       "      <td>[(New York-New Jersey-Long Island, NY-NJ, 6523...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Missing_Values  Unique_Values  \\\n",
       "ID                         0         852394   \n",
       "CNTYFIPS                   0           3079   \n",
       "Ori                        0          13442   \n",
       "State                      0             51   \n",
       "Agency                     0          10015   \n",
       "Agentype                   0              8   \n",
       "Source                     0              2   \n",
       "Solved                     0              2   \n",
       "Year                       0             48   \n",
       "Month                      0             12   \n",
       "Incident                   0           1805   \n",
       "ActionType                 0              2   \n",
       "Homicide                   0              2   \n",
       "Situation                  0              6   \n",
       "VicAge                     0            101   \n",
       "VicSex                     0              3   \n",
       "VicRace                    0              6   \n",
       "VicEthnic                  0              3   \n",
       "OffAge                     0            101   \n",
       "OffSex                     0              3   \n",
       "OffRace                    0              6   \n",
       "OffEthnic                  0              3   \n",
       "Weapon                     0             18   \n",
       "Relationship               0             29   \n",
       "Circumstance               0             34   \n",
       "Subcircum             859886              7   \n",
       "VicCount                   0             15   \n",
       "OffCount                   0             19   \n",
       "FileDate               17075           6244   \n",
       "MSA                        0            409   \n",
       "\n",
       "                                            Sample_Value_Counts          mean  \\\n",
       "ID            [(197702001NY05904, 13), (199204001FL05003, 12...           N/A   \n",
       "CNTYFIPS      [(Los Angeles, CA, 55681), (New York, NY, 4780...           N/A   \n",
       "Ori           [(NY03030, 47790), (ILCPD00, 30509), (CA01942,...           N/A   \n",
       "State         [(California, 129741), (Texas, 86288), (New Yo...           N/A   \n",
       "Agency        [(New York, 47790), (Chicago, 30509), (Los Ang...           N/A   \n",
       "Agentype      [(Municipal police, 692613), (Sheriff, 150999)...           N/A   \n",
       "Source                            [(FBI, 855718), (MAP, 38918)]           N/A   \n",
       "Solved                            [(Yes, 632457), (No, 262179)]           N/A   \n",
       "Year              [(1993, 24337), (1992, 23793), (2021, 23699)]   1998.868617   \n",
       "Month         [(July, 82444), (August, 80644), (December, 76...           N/A   \n",
       "Incident                  [(1, 364086), (2, 96879), (3, 53334)]     53.117632   \n",
       "ActionType      [(Normal update, 743918), (Adjustment, 150718)]           N/A   \n",
       "Homicide      [(Murder and non-negligent manslaughter, 87495...           N/A   \n",
       "Situation     [(Single victim/single offender, 489422), (Sin...           N/A   \n",
       "VicAge                  [(22, 31266), (25, 31172), (21, 30973)]     47.313424   \n",
       "VicSex        [(Male, 693403), (Female, 199567), (Unknown, 1...           N/A   \n",
       "VicRace       [(White, 435824), (Black, 427294), (Asian, 127...           N/A   \n",
       "VicEthnic     [(Unknown or not reported, 501898), (Not of Hi...           N/A   \n",
       "OffAge                [(999, 296295), (20, 28727), (19, 27048)]    351.740167   \n",
       "OffSex        [(Male, 559134), (Unknown, 264353), (Female, 7...           N/A   \n",
       "OffRace       [(Black, 304272), (White, 302973), (Unknown, 2...           N/A   \n",
       "OffEthnic     [(Unknown or not reported, 628333), (Not of Hi...           N/A   \n",
       "Weapon        [(Handgun - pistol, revolver, etc, 429715), (K...           N/A   \n",
       "Relationship  [(Relationship not determined, 354036), (Acqua...           N/A   \n",
       "Circumstance  [(Circumstances undetermined, 249629), (Other ...           N/A   \n",
       "Subcircum     [(Felon killed in commission of a crime, 11949...           N/A   \n",
       "VicCount                  [(0, 816622), (1, 56960), (2, 12398)]      0.134561   \n",
       "OffCount                  [(0, 780250), (1, 77211), (2, 22934)]      0.194908   \n",
       "FileDate      [(30180.0, 77684), (81723.0, 28487), (92324.0,...  54137.984079   \n",
       "MSA           [(New York-New Jersey-Long Island, NY-NJ, 6523...           N/A   \n",
       "\n",
       "                       std     min      25%      50%      75%       max  \\\n",
       "ID                     N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "CNTYFIPS               N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Ori                    N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "State                  N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Agency                 N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Agentype               N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Source                 N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Solved                 N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Year             14.134767  1976.0   1987.0   1997.0   2011.0    2023.0   \n",
       "Month                  N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Incident        790.036562     0.0      1.0      2.0     11.0   27113.0   \n",
       "ActionType             N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Homicide               N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Situation              N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "VicAge          117.434844     0.0     22.0     30.0     42.0     999.0   \n",
       "VicSex                 N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "VicRace                N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "VicEthnic              N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "OffAge          455.602403     0.0     24.0     38.0    999.0     999.0   \n",
       "OffSex                 N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "OffRace                N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "OffEthnic              N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Weapon                 N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Relationship           N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Circumstance           N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "Subcircum              N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "VicCount          0.605561     0.0      0.0      0.0      0.0      52.0   \n",
       "OffCount          0.617827     0.0      0.0      0.0      0.0      40.0   \n",
       "FileDate      32779.720549  1121.0  30180.0  41317.0  82207.0  123197.0   \n",
       "MSA                    N/A     N/A      N/A      N/A      N/A       N/A   \n",
       "\n",
       "              Highly_Correlated High_Variance Low_Variance  Potential_Index  \\\n",
       "ID                        False           N/A          N/A             True   \n",
       "CNTYFIPS                  False           N/A          N/A            False   \n",
       "Ori                       False           N/A          N/A            False   \n",
       "State                     False           N/A          N/A            False   \n",
       "Agency                    False           N/A          N/A            False   \n",
       "Agentype                  False           N/A          N/A            False   \n",
       "Source                    False           N/A          N/A            False   \n",
       "Solved                    False           N/A          N/A            False   \n",
       "Year                      False          True        False            False   \n",
       "Month                     False           N/A          N/A            False   \n",
       "Incident                  False          True        False            False   \n",
       "ActionType                False           N/A          N/A            False   \n",
       "Homicide                  False           N/A          N/A            False   \n",
       "Situation                 False           N/A          N/A            False   \n",
       "VicAge                    False          True        False            False   \n",
       "VicSex                    False           N/A          N/A            False   \n",
       "VicRace                   False           N/A          N/A            False   \n",
       "VicEthnic                 False           N/A          N/A            False   \n",
       "OffAge                    False          True        False            False   \n",
       "OffSex                    False           N/A          N/A            False   \n",
       "OffRace                   False           N/A          N/A            False   \n",
       "OffEthnic                 False           N/A          N/A            False   \n",
       "Weapon                    False           N/A          N/A            False   \n",
       "Relationship              False           N/A          N/A            False   \n",
       "Circumstance              False           N/A          N/A            False   \n",
       "Subcircum                 False           N/A          N/A            False   \n",
       "VicCount                  False         False        False            False   \n",
       "OffCount                  False         False        False            False   \n",
       "FileDate                  False          True        False            False   \n",
       "MSA                       False           N/A          N/A            False   \n",
       "\n",
       "             Data_Type  \n",
       "ID              object  \n",
       "CNTYFIPS        object  \n",
       "Ori             object  \n",
       "State           object  \n",
       "Agency          object  \n",
       "Agentype        object  \n",
       "Source          object  \n",
       "Solved          object  \n",
       "Year             int64  \n",
       "Month           object  \n",
       "Incident         int64  \n",
       "ActionType      object  \n",
       "Homicide        object  \n",
       "Situation       object  \n",
       "VicAge           int64  \n",
       "VicSex          object  \n",
       "VicRace         object  \n",
       "VicEthnic       object  \n",
       "OffAge           int64  \n",
       "OffSex          object  \n",
       "OffRace         object  \n",
       "OffEthnic       object  \n",
       "Weapon          object  \n",
       "Relationship    object  \n",
       "Circumstance    object  \n",
       "Subcircum       object  \n",
       "VicCount         int64  \n",
       "OffCount         int64  \n",
       "FileDate       float64  \n",
       "MSA             object  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Generate Data Quality Report 1 (DQR1)\n",
    "def generate_dqr1(df):\n",
    "    # Initialize an empty DataFrame to store DQR1 results\n",
    "    dqr1_df = pd.DataFrame(index=df.columns)\n",
    "    \n",
    "    # Basic Column Statistics\n",
    "    dqr1_df['Missing_Values'] = df.isnull().sum()\n",
    "    dqr1_df['Unique_Values'] = df.nunique()\n",
    "    \n",
    "    # Sample Value Counts (stored as a list to avoid multiple columns issue)\n",
    "    dqr1_df['Sample_Value_Counts'] = df.apply(lambda x: list(x.value_counts().head(3).to_dict().items()))\n",
    "\n",
    "    # Separate numeric and non-numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    non_numeric_df = df.select_dtypes(exclude=[np.number])\n",
    "\n",
    "    # Descriptive Statistics for Numeric Columns Only\n",
    "    descriptive_stats = numeric_df.describe().T\n",
    "    stats_columns = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    for col in stats_columns:\n",
    "        dqr1_df[col] = descriptive_stats[col]\n",
    "\n",
    "    # Convert stats columns to object type for safe assignment\n",
    "    dqr1_df[stats_columns] = dqr1_df[stats_columns].astype(object)\n",
    "    \n",
    "    # Mark irrelevant stats for non-numeric columns\n",
    "    dqr1_df.loc[non_numeric_df.columns, stats_columns] = 'N/A'\n",
    "\n",
    "    # Correlation Analysis for Numeric Columns Only\n",
    "    corr_matrix = numeric_df.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    highly_correlated = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
    "    dqr1_df['Highly_Correlated'] = dqr1_df.index.isin(highly_correlated)\n",
    "\n",
    "    # Variance Analysis for Numeric Columns Only\n",
    "    variances = numeric_df.var()\n",
    "    dqr1_df['High_Variance'] = dqr1_df.index.map(lambda x: variances[x] > 1.0 if x in variances else 'N/A')\n",
    "    dqr1_df['Low_Variance'] = dqr1_df.index.map(lambda x: variances[x] < 0.01 if x in variances else 'N/A')\n",
    "\n",
    "    # Potential Index Columns\n",
    "    dqr1_df['Potential_Index'] = dqr1_df['Unique_Values'] > len(df) * 0.9\n",
    "\n",
    "    # Additional Column Type Info\n",
    "    dqr1_df['Data_Type'] = df.dtypes\n",
    "\n",
    "    # Display the complete Data Quality Report with all rows and columns\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.options.display.max_columns = None\n",
    "    display(dqr1_df)\n",
    "\n",
    "    return dqr1_df\n",
    "\n",
    "# Generate the Data Quality Report\n",
    "dqr1_df = generate_dqr1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34247bf-8410-48ce-8a3f-c8b49cc0cf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treat_Flag</th>\n",
       "      <th>Outliers</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Encode</th>\n",
       "      <th>Missing_Values_Flag</th>\n",
       "      <th>Infinities_Flag</th>\n",
       "      <th>Correct_Data_Type</th>\n",
       "      <th>Correlated_Columns_Flag</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Data_Type</th>\n",
       "      <th>Potential_Issues</th>\n",
       "      <th>Recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0High cardinality (potential identifier).High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider dropping or indexing if needed.Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNTYFIPS</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ori</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agency</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agentype</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Solved</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>int64</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Incident</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>int64</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Outliers detected. High range detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider outlier treatment (e.g., capping, transformation). Consider scaling using StandardScaler or RobustScaler. Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ActionType</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Homicide</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Situation</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicAge</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>int64</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Outliers detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider outlier treatment (e.g., capping, transformation). Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicSex</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicRace</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicEthnic</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffAge</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>int64</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffSex</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffRace</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffEthnic</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weapon</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relationship</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Circumstance</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subcircum</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>Missing values: 2866286.67%\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider dropping or using advanced imputation if critical. Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VicCount</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>int64</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Outliers detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider outlier treatment (e.g., capping, transformation). Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OffCount</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>int64</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0 Outliers detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider outlier treatment (e.g., capping, transformation). Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FileDate</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>float64</td>\n",
       "      <td>Missing values: 56916.67%\\nRows with missing values exceeding 20%: 0 High range detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider dropping or using advanced imputation if critical. Consider scaling using StandardScaler or RobustScaler. Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSA</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Severity 1: Issues that must be addressed</td>\n",
       "      <td>object</td>\n",
       "      <td>\\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.</td>\n",
       "      <td>Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Treat_Flag  Outliers  Scale  Encode  Missing_Values_Flag  \\\n",
       "ID                  True     False  False    True                False   \n",
       "CNTYFIPS            True     False  False    True                False   \n",
       "Ori                 True     False  False    True                False   \n",
       "State               True     False  False    True                False   \n",
       "Agency              True     False  False    True                False   \n",
       "Agentype            True     False  False    True                False   \n",
       "Source              True     False  False    True                False   \n",
       "Solved              True     False  False    True                False   \n",
       "Year                True     False  False   False                False   \n",
       "Month               True     False  False    True                False   \n",
       "Incident            True      True   True   False                False   \n",
       "ActionType          True     False  False    True                False   \n",
       "Homicide            True     False  False    True                False   \n",
       "Situation           True     False  False    True                False   \n",
       "VicAge              True      True  False   False                False   \n",
       "VicSex              True     False  False    True                False   \n",
       "VicRace             True     False  False    True                False   \n",
       "VicEthnic           True     False  False    True                False   \n",
       "OffAge              True     False  False   False                False   \n",
       "OffSex              True     False  False    True                False   \n",
       "OffRace             True     False  False    True                False   \n",
       "OffEthnic           True     False  False    True                False   \n",
       "Weapon              True     False  False    True                False   \n",
       "Relationship        True     False  False    True                False   \n",
       "Circumstance        True     False  False    True                False   \n",
       "Subcircum           True     False  False    True                 True   \n",
       "VicCount            True      True  False   False                False   \n",
       "OffCount            True      True  False   False                False   \n",
       "FileDate            True     False   True   False                 True   \n",
       "MSA                 True     False  False    True                False   \n",
       "\n",
       "              Infinities_Flag  Correct_Data_Type  Correlated_Columns_Flag  \\\n",
       "ID                      False              False                    False   \n",
       "CNTYFIPS                False              False                    False   \n",
       "Ori                     False              False                    False   \n",
       "State                   False              False                    False   \n",
       "Agency                  False              False                    False   \n",
       "Agentype                False              False                    False   \n",
       "Source                  False              False                    False   \n",
       "Solved                  False              False                    False   \n",
       "Year                    False               True                    False   \n",
       "Month                   False              False                    False   \n",
       "Incident                False               True                    False   \n",
       "ActionType              False              False                    False   \n",
       "Homicide                False              False                    False   \n",
       "Situation               False              False                    False   \n",
       "VicAge                  False               True                    False   \n",
       "VicSex                  False              False                    False   \n",
       "VicRace                 False              False                    False   \n",
       "VicEthnic               False              False                    False   \n",
       "OffAge                  False               True                    False   \n",
       "OffSex                  False              False                    False   \n",
       "OffRace                 False              False                    False   \n",
       "OffEthnic               False              False                    False   \n",
       "Weapon                  False              False                    False   \n",
       "Relationship            False              False                    False   \n",
       "Circumstance            False              False                    False   \n",
       "Subcircum               False              False                    False   \n",
       "VicCount                False               True                    False   \n",
       "OffCount                False               True                    False   \n",
       "FileDate                False               True                    False   \n",
       "MSA                     False              False                    False   \n",
       "\n",
       "                                               Severity Data_Type  \\\n",
       "ID            Severity 1: Issues that must be addressed    object   \n",
       "CNTYFIPS      Severity 1: Issues that must be addressed    object   \n",
       "Ori           Severity 1: Issues that must be addressed    object   \n",
       "State         Severity 1: Issues that must be addressed    object   \n",
       "Agency        Severity 1: Issues that must be addressed    object   \n",
       "Agentype      Severity 1: Issues that must be addressed    object   \n",
       "Source        Severity 1: Issues that must be addressed    object   \n",
       "Solved        Severity 1: Issues that must be addressed    object   \n",
       "Year          Severity 1: Issues that must be addressed     int64   \n",
       "Month         Severity 1: Issues that must be addressed    object   \n",
       "Incident      Severity 1: Issues that must be addressed     int64   \n",
       "ActionType    Severity 1: Issues that must be addressed    object   \n",
       "Homicide      Severity 1: Issues that must be addressed    object   \n",
       "Situation     Severity 1: Issues that must be addressed    object   \n",
       "VicAge        Severity 1: Issues that must be addressed     int64   \n",
       "VicSex        Severity 1: Issues that must be addressed    object   \n",
       "VicRace       Severity 1: Issues that must be addressed    object   \n",
       "VicEthnic     Severity 1: Issues that must be addressed    object   \n",
       "OffAge        Severity 1: Issues that must be addressed     int64   \n",
       "OffSex        Severity 1: Issues that must be addressed    object   \n",
       "OffRace       Severity 1: Issues that must be addressed    object   \n",
       "OffEthnic     Severity 1: Issues that must be addressed    object   \n",
       "Weapon        Severity 1: Issues that must be addressed    object   \n",
       "Relationship  Severity 1: Issues that must be addressed    object   \n",
       "Circumstance  Severity 1: Issues that must be addressed    object   \n",
       "Subcircum     Severity 1: Issues that must be addressed    object   \n",
       "VicCount      Severity 1: Issues that must be addressed     int64   \n",
       "OffCount      Severity 1: Issues that must be addressed     int64   \n",
       "FileDate      Severity 1: Issues that must be addressed   float64   \n",
       "MSA           Severity 1: Issues that must be addressed    object   \n",
       "\n",
       "                                                                                                                                                                                                                             Potential_Issues  \\\n",
       "ID            \\nRows with missing values exceeding 20%: 0High cardinality (potential identifier).High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "CNTYFIPS                                              \\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Ori                                                   \\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "State                                                 \\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Agency                                                \\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Agentype                                                                                                                      \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Source                                                                                                                        \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Solved                                                                                                                        \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Year                                                                                                                                                          \\nRows with missing values exceeding 20%: 0\\nDataset contains 8 duplicate rows.   \n",
       "Month                                                                                                                         \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Incident                                                                                                              \\nRows with missing values exceeding 20%: 0 Outliers detected. High range detected.\\nDataset contains 8 duplicate rows.   \n",
       "ActionType                                                                                                                    \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Homicide                                                                                                                      \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Situation                                                                                                                     \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "VicAge                                                                                                                                     \\nRows with missing values exceeding 20%: 0 Outliers detected.\\nDataset contains 8 duplicate rows.   \n",
       "VicSex                                                                                                                        \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "VicRace                                                                                                                       \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "VicEthnic                                                                                                                     \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "OffAge                                                                                                                                                        \\nRows with missing values exceeding 20%: 0\\nDataset contains 8 duplicate rows.   \n",
       "OffSex                                                                                                                        \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "OffRace                                                                                                                       \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "OffEthnic                                                                                                                     \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Weapon                                                                                                                        \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Relationship                                                                                                                  \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Circumstance                                                                                                                  \\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "Subcircum                                                                                          Missing values: 2866286.67%\\nRows with missing values exceeding 20%: 0 Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "VicCount                                                                                                                                   \\nRows with missing values exceeding 20%: 0 Outliers detected.\\nDataset contains 8 duplicate rows.   \n",
       "OffCount                                                                                                                                   \\nRows with missing values exceeding 20%: 0 Outliers detected.\\nDataset contains 8 duplicate rows.   \n",
       "FileDate                                                                                                        Missing values: 56916.67%\\nRows with missing values exceeding 20%: 0 High range detected.\\nDataset contains 8 duplicate rows.   \n",
       "MSA                                                   \\nRows with missing values exceeding 20%: 0High unique values; may suffer from curse of dimensionality on encoding. Categorical data type detected.\\nDataset contains 8 duplicate rows.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                 Recommendations  \n",
       "ID                            Consider dropping or indexing if needed.Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "CNTYFIPS                                                              Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "Ori                                                                   Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "State                                                                 Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "Agency                                                                Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "Agentype                                                                                                                                                             Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "Source                                                                                                                                                               Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "Solved                                                                                                                                                               Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "Year                                                                                                                              Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.  \n",
       "Month                                                                                                                                                Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "Incident       Consider outlier treatment (e.g., capping, transformation). Consider scaling using StandardScaler or RobustScaler. Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.  \n",
       "ActionType                                                                                                                                                           Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "Homicide                                                                                                                                                             Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "Situation                                                                                                                                                            Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "VicAge                                                                Consider outlier treatment (e.g., capping, transformation). Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.  \n",
       "VicSex                                                                                                                                                               Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "VicRace                                                                                                                                                              Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "VicEthnic                                                                                                                                                            Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "OffAge                                                                                                                            Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.  \n",
       "OffSex                                                                                                                                                               Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "OffRace                                                                                                                                                              Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "OffEthnic                                                                                                                                                            Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "Weapon                                                                                                                                               Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "Relationship                                                                                                                                         Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "Circumstance                                                                                                                                         Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  \n",
       "Subcircum                                                                                                Consider dropping or using advanced imputation if critical. Use OneHotEncoder or pd.get_dummies for encoding. Consider removing duplicate rows to improve data quality.  \n",
       "VicCount                                                              Consider outlier treatment (e.g., capping, transformation). Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.  \n",
       "OffCount                                                              Consider outlier treatment (e.g., capping, transformation). Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.  \n",
       "FileDate       Consider dropping or using advanced imputation if critical. Consider scaling using StandardScaler or RobustScaler. Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer. Consider removing duplicate rows to improve data quality.  \n",
       "MSA                                                                   Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate. Use OrdinalEncoder or feature hashing to handle high cardinality. Consider removing duplicate rows to improve data quality.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Generate a Detailed Data Quality Report with Recommendations\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer, OneHotEncoder, OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def generate_data_quality_report_analysis(dqr1_df):\n",
    "    # Initialize a new DataFrame to store the analysis\n",
    "    analysis_df = dqr1_df[['Data_Type']].copy()  # Start with only Data_Type for essential columns\n",
    "\n",
    "    # Add columns for analysis\n",
    "    analysis_df['Treat_Flag'] = False  # Flag indicating if treatment is needed\n",
    "    analysis_df['Outliers'] = False    # Flag indicating if outliers are detected\n",
    "    analysis_df['Scale'] = False       # Flag for scaling recommendation\n",
    "    analysis_df['Encode'] = False      # Flag for encoding recommendation\n",
    "\n",
    "    # New True/False columns\n",
    "    analysis_df['Missing_Values_Flag'] = False\n",
    "    analysis_df['Infinities_Flag'] = False\n",
    "    analysis_df['Correct_Data_Type'] = True\n",
    "    analysis_df['Correlated_Columns_Flag'] = False\n",
    "\n",
    "    # Additional columns to be ordered last\n",
    "    analysis_df['Potential_Issues'] = \"\"  # Column to describe potential issues\n",
    "    analysis_df['Recommendations'] = \"\"  # Column to provide recommendations\n",
    "    analysis_df['Severity'] = \"Severity 3: Optional considerations for improved quality\"  # Default severity\n",
    "\n",
    "    # 1. Missing Values Analysis (including % missing by column)\n",
    "    missing_cols = dqr1_df[dqr1_df['Missing_Values'] > 0]\n",
    "    for col in missing_cols.index:\n",
    "        missing_percentage = (missing_cols['Missing_Values'][col] / len(dqr1_df)) * 100\n",
    "        analysis_df.loc[col, 'Missing_Values_Flag'] = True\n",
    "        analysis_df.loc[col, 'Potential_Issues'] += f\"Missing values: {missing_percentage:.2f}%\"\n",
    "        analysis_df.loc[col, 'Treat_Flag'] = True\n",
    "        analysis_df.loc[col, 'Severity'] = (\n",
    "            \"Severity 1: Issues that must be addressed\" if missing_percentage > 50\n",
    "            else \"Severity 2: Significant quality issues that impact model performance\"\n",
    "        )\n",
    "        if missing_percentage > 30:\n",
    "            analysis_df.loc[col, 'Recommendations'] += \"Consider dropping or using advanced imputation if critical.\"\n",
    "        else:\n",
    "            analysis_df.loc[col, 'Recommendations'] += \"Simple imputation recommended (mean/median for numeric, mode for categorical).\"\n",
    "\n",
    "    # Additional missing values analysis: missing by row\n",
    "    row_missing_percentage = (dqr1_df.isnull().sum(axis=1) / len(dqr1_df.columns)) * 100\n",
    "    analysis_df.loc[:, 'Potential_Issues'] += f\"\\nRows with missing values exceeding 20%: {sum(row_missing_percentage > 20)}\"\n",
    "    \n",
    "    # 2. Unique Values Analysis and Curse of Dimensionality Check\n",
    "    high_cardinality_cols = dqr1_df[dqr1_df['Potential_Index'] == True]\n",
    "    for col in high_cardinality_cols.index:\n",
    "        analysis_df.loc[col, 'Potential_Issues'] += \"High cardinality (potential identifier).\"\n",
    "        analysis_df.loc[col, 'Treat_Flag'] = True\n",
    "        analysis_df.loc[col, 'Recommendations'] += \"Consider dropping or indexing if needed.\"\n",
    "        analysis_df.loc[col, 'Severity'] = \"Severity 2: Significant quality issues that impact model performance\"\n",
    "\n",
    "    # Check for potential curse of dimensionality for categorical columns with high unique values\n",
    "    for col in analysis_df.index:\n",
    "        if dqr1_df['Data_Type'][col] == 'object' and dqr1_df['Unique_Values'][col] > 50:\n",
    "            analysis_df.loc[col, 'Potential_Issues'] += \"High unique values; may suffer from curse of dimensionality on encoding.\"\n",
    "            analysis_df.loc[col, 'Treat_Flag'] = True\n",
    "            analysis_df.loc[col, 'Encode'] = True\n",
    "            analysis_df.loc[col, 'Recommendations'] += \"Consider using OrdinalEncoder, feature hashing, or get_dummies if appropriate.\"\n",
    "            analysis_df.loc[col, 'Severity'] = \"Severity 2: Significant quality issues that impact model performance\"\n",
    "\n",
    "    # 3. Descriptive Statistics Analysis, Outlier Detection, and Scaling Recommendations\n",
    "    for col in analysis_df.index:\n",
    "        if dqr1_df['mean'][col] != 'N/A':\n",
    "            mean_val, min_val, max_val = dqr1_df['mean'][col], dqr1_df['min'][col], dqr1_df['max'][col]\n",
    "            iqr = dqr1_df['75%'][col] - dqr1_df['25%'][col]\n",
    "            lower_bound, upper_bound = dqr1_df['25%'][col] - 1.5 * iqr, dqr1_df['75%'][col] + 1.5 * iqr\n",
    "            # Check for outliers\n",
    "            if (max_val > upper_bound) or (min_val < lower_bound):\n",
    "                analysis_df.loc[col, 'Outliers'] = True\n",
    "                analysis_df.loc[col, 'Potential_Issues'] += \" Outliers detected.\"\n",
    "                analysis_df.loc[col, 'Recommendations'] += \" Consider outlier treatment (e.g., capping, transformation).\"\n",
    "                analysis_df.loc[col, 'Severity'] = \"Severity 2: Significant quality issues that impact model performance\"\n",
    "            # Scaling recommendation\n",
    "            if max_val - min_val > 1000:\n",
    "                analysis_df.loc[col, 'Scale'] = True\n",
    "                analysis_df.loc[col, 'Potential_Issues'] += \" High range detected.\"\n",
    "                analysis_df.loc[col, 'Recommendations'] += \" Consider scaling using StandardScaler or RobustScaler.\"\n",
    "                analysis_df.loc[col, 'Severity'] = \"Severity 2: Significant quality issues that impact model performance\"\n",
    "            # Check for infinities\n",
    "            if min_val == float('-inf') or max_val == float('inf'):\n",
    "                analysis_df.loc[col, 'Infinities_Flag'] = True\n",
    "                analysis_df.loc[col, 'Potential_Issues'] += \" Contains positive or negative infinity values.\"\n",
    "                analysis_df.loc[col, 'Treat_Flag'] = True\n",
    "                analysis_df.loc[col, 'Recommendations'] += \" Consider handling infinity values by replacing or capping.\"\n",
    "                analysis_df.loc[col, 'Severity'] = \"Severity 1: Issues that must be addressed\"\n",
    "            else:\n",
    "                # Additional scaler recommendations\n",
    "                analysis_df.loc[col, 'Recommendations'] += \" Alternative scaling options: MinMaxScaler, PowerTransformer, or QuantileTransformer.\"\n",
    "\n",
    "    # 4. Variance Analysis\n",
    "    low_variance_cols = dqr1_df[dqr1_df['Low_Variance'] == True]\n",
    "    for col in low_variance_cols.index:\n",
    "        analysis_df.loc[col, 'Potential_Issues'] += \" Low variance detected.\"\n",
    "        analysis_df.loc[col, 'Treat_Flag'] = True\n",
    "        analysis_df.loc[col, 'Recommendations'] += \" Consider dropping due to low variance.\"\n",
    "        analysis_df.loc[col, 'Severity'] = \"Severity 2: Significant quality issues that impact model performance\"\n",
    "\n",
    "    # 5. Correlation Analysis\n",
    "    highly_correlated_cols = dqr1_df[dqr1_df['Highly_Correlated'] == True]\n",
    "    for col in highly_correlated_cols.index:\n",
    "        analysis_df.loc[col, 'Correlated_Columns_Flag'] = True\n",
    "        analysis_df.loc[col, 'Potential_Issues'] += \" High correlation with other columns.\"\n",
    "        analysis_df.loc[col, 'Treat_Flag'] = True\n",
    "        analysis_df.loc[col, 'Recommendations'] += \" Consider removing redundant columns or using PCA.\"\n",
    "        analysis_df.loc[col, 'Severity'] = \"Severity 2: Significant quality issues that impact model performance\"\n",
    "\n",
    "    # 6. Data Type and Encoding Analysis\n",
    "    for col in analysis_df.index:\n",
    "        if analysis_df['Data_Type'][col] == 'object':\n",
    "            analysis_df.loc[col, 'Correct_Data_Type'] = False\n",
    "            analysis_df.loc[col, 'Potential_Issues'] += \" Categorical data type detected.\"\n",
    "            analysis_df.loc[col, 'Treat_Flag'] = True\n",
    "            analysis_df.loc[col, 'Encode'] = True\n",
    "            if dqr1_df['Unique_Values'][col] <= 10:\n",
    "                analysis_df.loc[col, 'Recommendations'] += \" Use OneHotEncoder or pd.get_dummies for encoding.\"\n",
    "            else:\n",
    "                analysis_df.loc[col, 'Recommendations'] += \" Use OrdinalEncoder or feature hashing to handle high cardinality.\"\n",
    "            analysis_df.loc[col, 'Severity'] = \"Severity 2: Significant quality issues that impact model performance\"\n",
    "        elif pd.api.types.is_numeric_dtype(analysis_df['Data_Type'][col]) and dqr1_df['Potential_Index'][col] == True:\n",
    "            analysis_df.loc[col, 'Potential_Issues'] += \" Potential index with high cardinality.\"\n",
    "            analysis_df.loc[col, 'Recommendations'] += \" Consider converting to `category` type to save memory.\"\n",
    "        elif analysis_df['Data_Type'][col] == 'object' and dqr1_df['Unique_Values'][col] > 50:\n",
    "            analysis_df.loc[col, 'Potential_Issues'] += \" Potential text column detected; consider for NLP analysis.\"\n",
    "            analysis_df.loc[col, 'Treat_Flag'] = True\n",
    "            analysis_df.loc[col, 'Recommendations'] += \" Consider using NLP techniques for text analysis.\"\n",
    "            analysis_df.loc[col, 'Severity'] = \"Severity 2: Significant quality issues that impact model performance\"\n",
    "\n",
    "    # 7. Duplicate Check (removing unhashable types)\n",
    "    hashable_df = dqr1_df.map(lambda x: x if isinstance(x, (str, int, float, bool, pd.Timestamp)) else None)\n",
    "    duplicate_rows = hashable_df.duplicated().sum()\n",
    "    if duplicate_rows > 0:\n",
    "        analysis_df.loc[:, 'Potential_Issues'] += f\"\\nDataset contains {duplicate_rows} duplicate rows.\"\n",
    "        analysis_df.loc[:, 'Treat_Flag'] = True\n",
    "        analysis_df.loc[:, 'Recommendations'] += \" Consider removing duplicate rows to improve data quality.\"\n",
    "        analysis_df.loc[:, 'Severity'] = \"Severity 1: Issues that must be addressed\"\n",
    "\n",
    "    # 8. Potential Index Columns Check\n",
    "    potential_index_cols = [col for col in dqr1_df.index if dqr1_df['Unique_Values'][col] == len(dqr1_df)]\n",
    "    if potential_index_cols:\n",
    "        for col in potential_index_cols:\n",
    "            analysis_df.loc[col, 'Potential_Issues'] += \" Unique values for each row; could serve as index.\"\n",
    "            analysis_df.loc[col, 'Recommendations'] += \" Consider using this column as an index.\"\n",
    "            analysis_df.loc[col, 'Severity'] = \"Severity 3: Optional considerations for improved quality\"\n",
    "\n",
    "    # Sort by 'Severity' and 'Treat_Flag' for priority order\n",
    "    analysis_df = analysis_df.sort_values(by=['Severity', 'Treat_Flag'], ascending=[True, False])\n",
    "\n",
    "    # Reorder columns: True/False flags first, followed by analysis columns, and ending with `Data_Type`, `Potential_Issues`, and `Recommendations`\n",
    "    column_order = [\n",
    "        'Treat_Flag', 'Outliers', 'Scale', 'Encode', 'Missing_Values_Flag', 'Infinities_Flag',\n",
    "        'Correct_Data_Type', 'Correlated_Columns_Flag', 'Severity', 'Data_Type', 'Potential_Issues', 'Recommendations'\n",
    "    ]\n",
    "    analysis_df = analysis_df[column_order]\n",
    "    \n",
    "    # Ensure Potential_Issues and Recommendations are fully readable\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    # Display the Data Quality Report Analysis\n",
    "    display(analysis_df)\n",
    "\n",
    "    return analysis_df\n",
    "\n",
    "# Generate the analysis report based on dqr1_df\n",
    "analysis_df = generate_data_quality_report_analysis(dqr1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68351e4-a48c-44bc-8722-09e3efc4d6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
